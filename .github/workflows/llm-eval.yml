name: LLM Eval (DeepEval + Bedrock Qwen)

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:  # Allow manual trigger

jobs:
  eval-basic:
    name: Basic Metrics (Relevancy, Faithfulness, Hallucination)
    runs-on: ubuntu-latest
    
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_SESSION_TOKEN: ${{ secrets.AWS_SESSION_TOKEN }}
      AWS_REGION: ap-south-1

    steps:
      - name: Check out repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install deepeval boto3 pytest

      - name: Run Basic Metrics Tests
        run: |
          deepeval test run test_qwen_eval.py -v

  eval-rag:
    name: RAG Metrics (Contextual Precision/Recall/Relevancy)
    runs-on: ubuntu-latest
    
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_SESSION_TOKEN: ${{ secrets.AWS_SESSION_TOKEN }}
      AWS_REGION: ap-south-1

    steps:
      - name: Check out repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install deepeval boto3 pytest

      - name: Run RAG Metrics Tests
        run: |
          deepeval test run test_rag_metrics.py -v

  eval-geval:
    name: Custom G-Eval Metrics
    runs-on: ubuntu-latest
    
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_SESSION_TOKEN: ${{ secrets.AWS_SESSION_TOKEN }}
      AWS_REGION: ap-south-1

    steps:
      - name: Check out repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install deepeval boto3 pytest

      - name: Run G-Eval Metrics Tests
        run: |
          deepeval test run test_geval_metrics.py -v

  eval-safety:
    name: Safety Metrics (Bias, Toxicity)
    runs-on: ubuntu-latest
    
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_SESSION_TOKEN: ${{ secrets.AWS_SESSION_TOKEN }}
      AWS_REGION: ap-south-1

    steps:
      - name: Check out repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install deepeval boto3 pytest

      - name: Run Safety Metrics Tests
        run: |
          deepeval test run test_safety_metrics.py -v

  eval-dataset:
    name: Dataset-Based Evaluation
    runs-on: ubuntu-latest
    
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_SESSION_TOKEN: ${{ secrets.AWS_SESSION_TOKEN }}
      AWS_REGION: ap-south-1

    steps:
      - name: Check out repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install deepeval boto3 pytest

      - name: Run Dataset Evaluation Tests
        run: |
          deepeval test run test_dataset_eval.py -v

  summary:
    name: Evaluation Summary
    runs-on: ubuntu-latest
    needs: [eval-basic, eval-rag, eval-geval, eval-safety, eval-dataset]
    if: always()
    
    steps:
      - name: Check Results
        run: |
          echo "=== LLM Evaluation Summary ==="
          echo "Basic Metrics: ${{ needs.eval-basic.result }}"
          echo "RAG Metrics: ${{ needs.eval-rag.result }}"
          echo "G-Eval Metrics: ${{ needs.eval-geval.result }}"
          echo "Safety Metrics: ${{ needs.eval-safety.result }}"
          echo "Dataset Evaluation: ${{ needs.eval-dataset.result }}"
          
          # Fail if any job failed
          if [ "${{ needs.eval-basic.result }}" == "failure" ] || \
             [ "${{ needs.eval-rag.result }}" == "failure" ] || \
             [ "${{ needs.eval-geval.result }}" == "failure" ] || \
             [ "${{ needs.eval-safety.result }}" == "failure" ] || \
             [ "${{ needs.eval-dataset.result }}" == "failure" ]; then
            echo "Some evaluations failed!"
            exit 1
          fi
          
          echo "All evaluations passed!"
